# Prediction #2: Energy-Complexity Scaling Laws

**Status:** Protocol complete, awaiting empirical validation

## Quick Summary

**Hypothesis:** Energy consumption scales differently across substrates depending on algorithm complexity class.

**Key Predictions:**
- Linear O(n) algorithms: Neuromorphic shows linear advantage growth
- Quadratic O(n²) algorithms: Neuromorphic shows superlinear advantage  
- Logarithmic O(log n) algorithms: Silicon performs equal or better

**Why this matters:** Provides quantitative guidance for substrate-algorithm matching and hybrid system design.

## Files

- **[protocol.md](protocol.md)** - Complete experimental protocol (~7,000 words)
- **rationale.md** - Theoretical basis and importance (coming soon)
- **code/** - Implementation scripts (coming soon)
- **results/** - Empirical findings (awaiting experiments)

## Status

- ✅ Theoretical predictions: Complete
- ✅ Detailed protocol: Complete  
- ⏳ Code implementations: In progress
- ⏳ Empirical validation: Not yet started
- ⏳ Pre-registration: Open for contributors

## How to Test This Prediction

1. Read the [detailed protocol](protocol.md)
2. Implement algorithms on both silicon and neuromorphic hardware
3. Measure energy following protocol
4. Compare results to predictions
5. Share findings via Pull Request

**Need help?** Open an [Issue](../../../../issues) with tag `prediction-2`
